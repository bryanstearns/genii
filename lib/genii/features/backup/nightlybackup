#!/usr/bin/env ruby
#
# Make a nightly backup, based on individual configuration files of things
# to be backed up or archived, eg:
# - database backups
# - system, apache, mail, rails logs
# - RRD databases
# - wiki content
# - hudson build history & stats
#
# There's a plugin mechanism to decide how to deal with each kind of thing,
# and a configuration directory to configure the specific instances.
#
# This script manages a filesystem hierarchy (by default in
# /var/spool/nightlybackup) of date-stamped directories containing a rotating
# set of recent backups. Each day's backup is also copied to a bucket on S3
# (if a keys are configured); the three most recent ones are kept there.
# Individual instances can also decide to keep archives on S3: we use this to
# move all but the most recent system & Rails-app logs there.
#
# The plugin files are .rb files subclassed from BackupItem; each configuration
# file can be a .yml set of initialization options for a BackupItem instance,
# or a one-off BackupItem subclass in an .rb file (really just a vestige from
# before the plugin.d directory existed).
#

require 'fileutils'
require 'yaml'
require 'syslog'
require 'optparse'
require 'tempfile'
require 'rubygems'
require 's3'
require 'ruby-debug'

def log(msg)
  if STDOUT.isatty
    puts msg
    STDOUT.flush
  else
    Syslog.open('nightlybackup', Syslog::LOG_PID | Syslog::LOG_CONS) \
      unless Syslog.opened?
    Syslog.log(Syslog::LOG_INFO, msg)
  end
end

def die(msg)
  log(msg)
  exit(1)
end

class String
  def classify
    # Fake ActiveSupport's classify:
    # murky_soup --> MurkySoup
    self.downcase.split('_').map do |word|
      word[0..0].upcase + word[1..-1]
    end.join
  end

  def constantize
    # Fake ActiveSupport's constantize
    names = self.split('::')
    constant = Object
    names.each do |name|
      constant = constant.const_defined?(name) ? constant.const_get(name) : constant.const_missing(name)
    end
    constant
  end
end

module Execute
  def execute(command)
    # Run a shell command in a subprocess and return its standard output
    output = IO.popen("(#{command}) 2>&1") do |f|
      if block_given?
        yield f
        f.close_write
      end
      f.read
    end
  end
end

class BackupItem
  # One thing to be backed up.
  include Execute
  attr_accessor :name, :root_dir, :exclude

  def initialize(context, options)
    @context = context
    options.each {|k, v| send("#{k}=", v) }
  end

  def run
    # A subclass will likely override this and not call super
    puts "Running #{inspect}"
    exclusions = exceptions.map{|e| " --exclude=#{e}"}.join(' ') \
      if exclude
    cmd = [
      "cd #{root_dir} && tar czf -#{exclusions}",
      @context.encrypt_command(:out => pathname(".tgz.enc"))
    ].join(' | ')
    log(cmd)
    output = execute(cmd)
    abort "Backup of filesystem #{root_dir} failed (#{$?}): #{output}" unless $? == 0
    log("Backed up filesystem: #{root_dir}")
  end

  def verbose
    @context.verbose
  end

  def pathname(extension=nil)
    # guess at an appropriate path to back up one thing for this item
    File.join(@context.latest_dir, "#{name}#{extension}")
  end
end

class NightlyBackup
  include Execute

  # Values we load from our configuration file
  attr_accessor :hostname, :domain, 
                :s3_access_key_id, :s3_secret_access_key,
                :encryption_key_path, :backup_dir, :verbose,
                :skip_s3, :skip_cleanup

  # Other values
  attr_accessor :items, :configuration_root

  def initialize(*args)
    @configuration_root ||= "/etc/nightlybackup"
    @backup_dir ||= "/var/spool/nightlybackup"
    parse_options(args)

    read_configuration
    load_plugins
    read_configured_items
  end

  def run!
    if items.empty?
      log "Nothing to do: no configured backup items!"
      exit(0)
    end

    create_latest_folder
    backup_items
    if s3_access_key_id && !skip_s3
      copy_latest_to_s3
      cleanup_s3_backups unless skip_cleanup
    end
    cleanup_local_backups unless skip_cleanup
    0
  end

  def parse_options(args)
    OptionParser.new do |opts|
      opts.banner = """nightlybackup - do nightly backup processing

Usage: nightlybackup [options]
"""

      opts.on("--config FILE", "Use configuration from here, instead of #{@configuration_root}")\
        {|@configuration_root|}
      opts.on("--backup_dir NAME", "Backup to a hierarchy rooted here (instead of #{@backup_dir})")\
        {|@backup_dir|}
      opts.on("--force", "Overwrite today's backup if it exists already")\
        {|@force|}
      opts.on("--skip_s3", "Don't copy to S3 or clean it up")\
        {|@skip_s3|}
      opts.on("--skip_cleanup", "Don't do any cleanup")\
        {|@skip_cleanup|}
      opts.on("--today DATE", "pretend it's this date (for testing only)") do |today|
        @today = Date.parse(today).strftime("%y%m%d%a").downcase # "090817mon"
      end
      opts.on("-v", "--verbose", "Blah blah blah")\
        {|@verbose|}
      opts.on_tail("-h", "--help", "Show this message") do
        puts opts
        return 2
      end
    end.parse!(args)
  end

  def read_configuration
    configuration = load_yaml(configuration_file)
    configuration.each do |k, v|
      log("#{k} < #{v}") if verbose
      send("#{k}=", v)
    end
  end

  def load_plugins
    $LOAD_PATH << plugin_dir \
      unless $LOAD_PATH.include?(plugin_dir)
    Dir.glob("#{plugin_dir}/*.rb") do |path|
      name = File.basename(path, ".rb")
      log("requiring plugin #{name}") if verbose
      require name
    end
  end

  def read_configured_items
    @items = []
    $LOAD_PATH << configuration_dir \
      unless $LOAD_PATH.include?(configuration_dir)
    Dir.glob("#{configuration_dir}/*") do |path|
      name, extension = /([^\/]+)\.([^\.]+)$/.match(path)[1,2]
      item = case extension
      when "rb"
        log("requiring #{name}") if verbose
        require name
        name.classify.constantize.new(self)
      when "yml"
        log("reading #{path}") if verbose
        options = load_yaml(path)
        options["name"] ||= name
        klass = options.delete("class")
        klass = klass ? klass.classify.constantize : BackupItem
        klass.new(self, options)
      end
      @items << item
    end
  end

  def create_latest_folder
    log("making latest dir: #{latest_dir}") if verbose
    if File.directory?(latest_dir)
      die("Today's backup already exists: #{latest_dir}") \
        unless @force
      FileUtils.rm_rf(latest_dir)
    end
    FileUtils.mkdir_p(latest_dir)
  end

  def backup_items
    @items.each {|item| item.run }
  end

  def copy_latest_to_s3
    files = Dir.glob("#{latest_dir}/*")
    if files.empty?
      log "Nothing to copy to s3 from #{latest_dir}"
    else
      s3_bucket_add(today, files)
    end
  end

  def cleanup_s3_backups
    # Just keep this many of the most recent daily backups
    log "--\nS3 cleanup:"
    keep_count = 3
    buckets_by_date = s3.buckets.inject({}) do |h, bucket|
      if bucket.name =~ /^backup\.(\d\d\d\d\d\d\S\S\S).#{hostname}/
        h[$~[1]] = bucket
      end
      h
    end
    if buckets_by_date.size > keep_count
      log "Have #{buckets_by_date.size} recent backups on S3: purging"
      sorted_dates = buckets_by_date.keys.sort
      sorted_dates[0...-keep_count].each do |date|
        bucket = buckets_by_date[date]
        log "-- Discarding #{bucket.name}" if verbose
        bucket.objects.each do |object|
          object.destroy
        end
        bucket.destroy
      end
      sorted_dates[-keep_count..-1].each do |date|
        log "-- Keeping #{buckets_by_date[date].name}" if verbose
      end
    else
      log "Only have #{buckets_by_date.size} recent backups on S3: not purging"
    end
  end

  def cleanup_local_backups
    # Clean up old backups
    # - Keep all from the last two weeks
    # - Keep all Mondays less than two months old
    # - Keep all second Mondays of the month (used to be first Monday, but this
    #   increases the chance that we'll get most of the month's rental action)
    # - Keep everything until we've got backups from the most recent two weeks.
    doomed = []
    recent_count = 0
    log "--\nLocal cleanup:"
    Dir.entries(backup_dir).sort.each do |f|
      next unless f =~ /(\d\d)(\d\d)(\d\d)(\S\S\S)$/
      ignored, year, month, day, day_of_week = $~.to_a
      timestamp = Time.local(year.to_i, month.to_i, day.to_i, 0, 0, 0, 0)
      days_old = (Time.now - timestamp) / 86400
      if days_old <= 14
        log "-- Keeping #{f}: less than two weeks old."
        recent_count += 1
        next
      end
      if day_of_week == 'mon'
        if days_old <= 62
          log "-- Keeping #{f}: a Monday less than about two months old."
          next
        end
        if [8..15].include? day
          log "-- Keeping #{f}: a second Monday of the month."
          next
        end
      end
      log "-- Deleting #{f}"
      doomed << f
    end

    if recent_count < 14
      log "-- .. postponing cleanup: recent backup count is #{recent_count}"
    else
      doomed.map {|f| FileUtils.rm("#{backup_dir}/#{f}") }
    end
  end

  def today
    # We downcase because s3's bucket naming disallows uppercase
    @today ||= Time.now.strftime("%y%m%d%a").downcase # "090817mon"
  end

  def latest_dir
    "#{backup_dir}/#{today}"
  end


  def plugin_dir
    "#{configuration_root}/plugin.d"
  end

  def configuration_dir
    "#{configuration_root}/conf.d"
  end

  def configuration_file
    "#{configuration_root}/nightlybackup.yml"
  end

  def load_yaml(path)
    log("loading configuration #{path}") if verbose
    result = File.open(path) {|f| YAML::load(f) }
    result = {} if result == false # the file is empty?!
    result
  end

  def encrypt_command(options={})
    cmd = [ "/usr/local/bin/encdec --key #{encryption_key_path}" ]
    cmd << "--in #{options[:in]}" if options[:in]
    cmd << "--out #{options[:out]}" if options[:out]
    cmd.join(' ')
  end

  def database_dump_command(database, user=nil, password=nil)
    user_option = "-u#{user} " if user
    password_option = "-p#{password} " if password
    "mysqldump --single-transaction --database #{database} " +
      "#{user_option}#{password_option} --opt --skip-extended-insert"
  end

  def s3
    @s3 ||= S3::Service.new(:access_key_id => s3_access_key_id,
                            :secret_access_key => s3_secret_access_key,
                            :use_ssl => true) #, :debug => true)
  end

  def hostname
    @hostname ||= `hostname --fqdn`.strip
  end

  def s3_bucket_name_expansion(name)
    "backup.#{name}.#{hostname}"
  end

  def s3_bucket(bucket_name, options=nil)
    options ||= {}
    bucket_name = s3_bucket_name_expansion(bucket_name)
    @s3_buckets ||= {}
    bucket = @s3_buckets[bucket_name]
    unless bucket
      bucket = begin
        log("Looking for bucket #{bucket_name}") if verbose
        s3.buckets.find(bucket_name)
      rescue S3::Error::NoSuchBucket
        nil
      end
      if !bucket
        return nil unless options[:create]
        bucket = @s3.buckets.build(bucket_name)
        bucket.save(:us)
      end
      @s3_buckets[bucket_name] = bucket
    end
    bucket
  end

  def s3_bucket_contains?(bucket_name, name)
    bucket = s3_bucket(bucket_name)
    bucket && bucket.objects.find_first(name) 
  rescue S3::Error::NoSuchKey
    nil
  end

  def s3_bucket_add(bucket_name, paths, options=nil)
    paths = [paths].flatten
    options ||= {}
    bucket = nil
    paths.each do |path|
      name = options[:name] || File.basename(path)
      if !options[:unless_exists] || s3_bucket_contains?(bucket_name, name)
        bucket ||= s3_bucket(bucket_name, options.merge(:create => true))
        object = bucket.objects.build(name)
        how_much = human_size(File.stat(path).size)
        log("Copying #{path} to #{object.url} (#{how_much})")
        File.open(path) do |f|
          object.content = f
          object.save
        end
      end
    end
  end

  def human_size(bytes)
    units = %w[B KB MB GB TB]
    e = (Math.log(bytes) / Math.log(1024)).floor
    s = "%.3f" % (bytes.to_f / 1024**e)
    s.sub(/\.?0*$/, units[e])
  end
end

exit NightlyBackup.new(*ARGV).run!
